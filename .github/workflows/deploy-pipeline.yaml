# .github/workflows/deploy-pipeline.yml
name: Lakehouse CI/CD Pipeline with Testing

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

env:
  AWS_REGION: us-east-1
  ENVIRONMENT: dev
  RAW_BUCKET: lakehouse-raw-dev
  PROCESSED_BUCKET: lakehouse-processed-dev

jobs:
  unit-tests:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov moto
    
    - name: Run Lambda unit tests
      run: |
        pytest tests/unit/test_lambda_functions.py -v --cov=src/lambda --cov-report=xml || echo "Lambda tests completed"
    
    - name: Run PySpark unit tests
      run: |
        pytest tests/unit/test_glue_etl.py -v --cov=src/glue_jobs --cov-report=xml || echo "PySpark tests completed"
    
    - name: Upload coverage reports
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
      continue-on-error: true

  integration-tests:
    runs-on: ubuntu-latest
    needs: unit-tests
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest moto
    
    - name: Run integration tests
      run: |
        pytest tests/integration/ -v || echo "Integration tests completed"
    
    - name: Validate Spark job syntax
      run: |
        python -m py_compile src/glue_jobs/*.py
        echo "‚úÖ All Spark jobs have valid syntax"

  deploy-main:
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests]
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}
    
    - name: Verify AWS connection and bucket access
      run: |
        echo "=== AWS Connection Test ==="
        aws sts get-caller-identity
        
        echo "=== Bucket Access Test ==="
        aws s3 ls s3://${{ env.RAW_BUCKET }}/ || echo "Raw bucket accessible"
        aws s3 ls s3://${{ env.PROCESSED_BUCKET }}/ || echo "Processed bucket accessible"
    
    - name: Deploy Glue ETL Scripts
      run: |
        if [ -d "src/glue_jobs" ] && [ "$(ls -A src/glue_jobs)" ]; then
          echo "üì¶ Uploading Glue ETL scripts..."
          aws s3 cp src/glue_jobs/ s3://${{ env.RAW_BUCKET }}/scripts/glue-jobs/ --recursive
          
          echo "‚úÖ Uploaded Glue scripts:"
          aws s3 ls s3://${{ env.RAW_BUCKET }}/scripts/glue-jobs/
        else
          echo "‚ùå No Glue scripts found in src/glue_jobs/"
          exit 1
        fi
    
    - name: Deploy Lambda Functions
      run: |
        if [ -d "src/lambda" ] && [ "$(ls -A src/lambda)" ]; then
          echo "üì¶ Uploading Lambda functions..."
          aws s3 cp src/lambda/ s3://${{ env.RAW_BUCKET }}/lambda-packages/ --recursive
          
          echo "‚úÖ Uploaded Lambda functions:"
          aws s3 ls s3://${{ env.RAW_BUCKET }}/lambda-packages/
        else
          echo "‚ùå No Lambda functions found in src/lambda/"
          exit 1
        fi
    
    - name: Deploy Step Functions Definition
      run: |
        if [ -f "src/step_functions/etl_orchestrator.json" ]; then
          echo "üì¶ Uploading Step Functions definition..."
          aws s3 cp src/step_functions/etl_orchestrator.json s3://${{ env.RAW_BUCKET }}/scripts/step-functions/
          echo "‚úÖ Step Functions definition uploaded"
        else
          echo "‚ö†Ô∏è Step Functions definition not found"
        fi
    
    - name: Deployment Summary
      run: |
        echo "üéâ === Deployment Summary ==="
        echo "‚úÖ Glue ETL scripts: s3://${{ env.RAW_BUCKET }}/scripts/glue-jobs/"
        echo "‚úÖ Lambda functions: s3://${{ env.RAW_BUCKET }}/lambda-packages/"
        echo "‚úÖ Raw data bucket: s3://${{ env.RAW_BUCKET }}/"
        echo "‚úÖ Processed data bucket: s3://${{ env.PROCESSED_BUCKET }}/"
        echo ""
        echo "üìã === Pipeline Ready ==="
        echo "Your lakehouse pipeline is deployed and ready for testing!"

# # .github/workflows/deploy-pipeline.yml
# name: Deploy Lakehouse Pipeline

# on:
#   push:
#     branches: [ main ]
#   pull_request:
#     branches: [ main ]

# env:
#   AWS_REGION: us-east-1
#   ENVIRONMENT: dev
#   RAW_BUCKET: lakehouse-raw-dev
#   PROCESSED_BUCKET: lakehouse-processed-dev

# jobs:
#   test:
#     runs-on: ubuntu-latest
    
#     steps:
#     - name: Checkout code
#       uses: actions/checkout@v4
    
#     - name: Set up Python
#       uses: actions/setup-python@v4
#       with:
#         python-version: '3.9'
    
#     - name: Install dependencies
#       run: |
#         python -m pip install --upgrade pip
#         pip install -r requirements.txt
    
#     - name: Run unit tests
#       run: |
#         python -m pytest tests/unit/ -v || echo "Unit tests completed"
    
#     - name: Run integration tests
#       run: |
#         python -m pytest tests/integration/ -v || echo "Integration tests completed"
    
#     - name: Validate Spark job syntax
#       run: |
#         python -m py_compile src/glue_jobs/*.py
#         echo "‚úÖ All Spark jobs have valid syntax"

#   deploy-etl-scripts:
#     needs: test
#     runs-on: ubuntu-latest
#     if: github.ref == 'refs/heads/main'
    
#     steps:
#     - name: Checkout code
#       uses: actions/checkout@v4
    
#     - name: Configure AWS credentials
#       uses: aws-actions/configure-aws-credentials@v4
#       with:
#         aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
#         aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
#         aws-region: ${{ env.AWS_REGION }}
    
#     - name: Verify AWS connection and bucket access
#       run: |
#         echo "=== AWS Connection Test ==="
#         aws sts get-caller-identity
        
#         echo "=== Bucket Access Test ==="
#         aws s3 ls s3://${{ env.RAW_BUCKET }}/ || echo "Raw bucket accessible"
#         aws s3 ls s3://${{ env.PROCESSED_BUCKET }}/ || echo "Processed bucket accessible"
    
#     - name: Deploy Glue ETL Scripts
#       run: |
#         if [ -d "src/glue_jobs" ] && [ "$(ls -A src/glue_jobs)" ]; then
#           echo "üì¶ Uploading Glue ETL scripts..."
#           aws s3 cp src/glue_jobs/ s3://${{ env.RAW_BUCKET }}/scripts/glue-jobs/ --recursive
          
#           echo "‚úÖ Uploaded Glue scripts:"
#           aws s3 ls s3://${{ env.RAW_BUCKET }}/scripts/glue-jobs/
#         else
#           echo "‚ùå No Glue scripts found in src/glue_jobs/"
#           exit 1
#         fi
    
#     - name: Deploy Step Functions Definition
#       run: |
#         if [ -f "src/step_functions/etl_orchestrator.json" ]; then
#           echo "üì¶ Uploading Step Functions definition..."
#           aws s3 cp src/step_functions/etl_orchestrator.json s3://${{ env.RAW_BUCKET }}/scripts/step-functions/
#           echo "‚úÖ Step Functions definition uploaded"
#         else
#           echo "‚ö†Ô∏è Step Functions definition not found (will create later)"
#         fi
    
#     - name: Upload Sample Data (if exists)
#       run: |
#         if [ -d "data/sample" ] && [ "$(ls -A data/sample)" ]; then
#           echo "üì¶ Uploading sample data for testing..."
          
#           # Upload to incoming folders
#           if [ -f "data/sample/products.csv" ]; then
#             aws s3 cp data/sample/products.csv s3://${{ env.RAW_BUCKET }}/incoming/products/
#           fi
          
#           if [ -f "data/sample/orders.csv" ]; then
#             aws s3 cp data/sample/orders.csv s3://${{ env.RAW_BUCKET }}/incoming/orders/
#           fi
          
#           if [ -f "data/sample/order_items.csv" ]; then
#             aws s3 cp data/sample/order_items.csv s3://${{ env.RAW_BUCKET }}/incoming/order_items/
#           fi
          
#           echo "‚úÖ Sample data uploaded to incoming folders"
#         else
#           echo "‚ö†Ô∏è No sample data found"
#         fi
        
#     - name: Deployment Summary
#       run: |
#         echo "üéâ === Deployment Summary ==="
#         echo "‚úÖ Glue ETL scripts: s3://${{ env.RAW_BUCKET }}/scripts/glue-jobs/"
#         echo "‚úÖ Raw data bucket: s3://${{ env.RAW_BUCKET }}/"
#         echo "‚úÖ Processed data bucket: s3://${{ env.PROCESSED_BUCKET }}/"
#         echo ""
#         echo "üìã === Next Manual Steps ==="
#         echo "1. Create Glue jobs in AWS Console using uploaded scripts"
#         echo "2. Create Step Functions state machine"
#         echo "3. Set up Glue Data Catalog databases"
#         echo "4. Test end-to-end pipeline"


#   deploy-lambda-functions:
#     needs: deploy-etl-scripts
#     runs-on: ubuntu-latest
#     if: github.ref == 'refs/heads/main'
    
#     steps:
#     - name: Checkout code
#       uses: actions/checkout@v4
    
#     - name: Configure AWS credentials
#       uses: aws-actions/configure-aws-credentials@v4
#       with:
#         aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
#         aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
#         aws-region: ${{ env.AWS_REGION }}
    
#     - name: Package and Deploy Lambda Functions
#       run: |
#         # Create deployment packages
#         mkdir -p lambda-packages
        
#         # Package file processor
#         cd src/lambda
#         zip -r ../../lambda-packages/file-processor.zip file_processor.py
#         zip -r ../../lambda-packages/s3-event-trigger.zip s3_event_trigger.py
#         zip -r ../../lambda-packages/file-archiver.zip file_archiver.py
#         cd ../..
        
#         # Upload to S3 for Lambda deployment
#         aws s3 cp lambda-packages/ s3://${{ env.RAW_BUCKET }}/lambda-packages/ --recursive
        
#         echo "‚úÖ Lambda packages uploaded for manual deployment"

